{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dec8fea-1741-40be-bc89-be8043606a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenized tensors from tokenized_logs.pt ...\n",
      "Found 7000 rows, seq_len=128\n",
      "Loading encoder model 'distilroberta-base' to device=cuda ...\n",
      "Processed 640/7000 rows\n",
      "Processed 1280/7000 rows\n",
      "Processed 1920/7000 rows\n",
      "Processed 2560/7000 rows\n",
      "Processed 3200/7000 rows\n",
      "Processed 3840/7000 rows\n",
      "Processed 4480/7000 rows\n",
      "Processed 5120/7000 rows\n",
      "Processed 5760/7000 rows\n",
      "Processed 6400/7000 rows\n",
      "Processed 7000/7000 rows\n",
      "Embeddings shape: (7000, 768)\n",
      "Saved embeddings to: embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "# make_embeddings_from_tokenized.py\n",
    "# Produces embeddings.npy (row-aligned) from tokenized_logs.pt\n",
    "# Requires: torch, transformers, numpy\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "TOKENIZED_PT = \"tokenized_logs.pt\"   # your file\n",
    "EMBED_OUT = \"embeddings.npy\"\n",
    "MODEL_NAME = \"distilroberta-base\"\n",
    "BATCH_SIZE = 64    # lower this if you run out of GPU memory\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PRINT_EVERY = 10\n",
    "\n",
    "def main():\n",
    "    assert os.path.exists(TOKENIZED_PT), f\"tokenized file not found: {TOKENIZED_PT}\"\n",
    "    print(f\"Loading tokenized tensors from {TOKENIZED_PT} ...\")\n",
    "    data = torch.load(TOKENIZED_PT, weights_only=False)\n",
    "\n",
    "    # Expect keys input_ids, attention_mask (and optionally labels)\n",
    "    if \"input_ids\" not in data or \"attention_mask\" not in data:\n",
    "        raise KeyError(\"tokenized file must contain 'input_ids' and 'attention_mask' tensors\")\n",
    "\n",
    "    input_ids = data[\"input_ids\"]\n",
    "    attention_mask = data[\"attention_mask\"]\n",
    "\n",
    "    # convert to torch tensors on cpu (DataLoader will move to device)\n",
    "    if not isinstance(input_ids, torch.Tensor):\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "    if not isinstance(attention_mask, torch.Tensor):\n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    n_rows = input_ids.size(0)\n",
    "    print(f\"Found {n_rows} rows, seq_len={input_ids.size(1)}\")\n",
    "\n",
    "    # DataLoader\n",
    "    ds = TensorDataset(input_ids, attention_mask)\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Load model\n",
    "    print(f\"Loading encoder model '{MODEL_NAME}' to device={DEVICE} ...\")\n",
    "    model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # We'll collect embeddings in a list then stack\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            batch_input_ids = batch[0].to(DEVICE)\n",
    "            batch_attention_mask = batch[1].to(DEVICE)\n",
    "\n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "            # last_hidden_state shape: (batch, seq_len, hidden)\n",
    "            last_hidden = outputs.last_hidden_state\n",
    "            # CLS token (DistilRoBERTa uses first token) -> index 0\n",
    "            cls_emb = last_hidden[:, 0, :].detach().cpu().numpy()  # shape (batch, hidden)\n",
    "            embeddings.append(cls_emb)\n",
    "\n",
    "            if (i + 1) % PRINT_EVERY == 0:\n",
    "                print(f\"Processed {min((i+1)*BATCH_SIZE, n_rows)}/{n_rows} rows\")\n",
    "\n",
    "    embeddings = np.vstack(embeddings)  # shape (n_rows, hidden)\n",
    "    assert embeddings.shape[0] == n_rows, f\"Mismatch rows: {embeddings.shape[0]} vs {n_rows}\"\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "    # Optionally L2-normalize rows (your PPO script normalizes again; do if you prefer)\n",
    "    # norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    # norms[norms == 0] = 1.0\n",
    "    # embeddings = embeddings / norms\n",
    "\n",
    "    # Save .npy\n",
    "    np.save(EMBED_OUT, embeddings)\n",
    "    print(f\"Saved embeddings to: {EMBED_OUT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d6fd269-e9c7-43ec-b248-6a1aee7a1abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Loading embeddings...\n",
      "    embeddings shape: (7000, 768)\n",
      "[*] Loading logs CSV...\n",
      "[*] Loading reconstruction errors CSV...\n",
      "[*] Embeddings L2-normalized\n",
      "[*] Running PCA -> 30 components ...\n",
      "    PCA done. Explained variance sum: 0.9218317270278931\n",
      "[*] PCA standardized and scaled to [-1,1]\n",
      "[*] Computing pattern scores from query+uri ...\n",
      "[*] Concatenating features into observations (PCA20 + recon + rule) ...\n",
      "[+] Saved observations to: observations_22d.npy  (shape (7000, 32))\n",
      "[+] Saved PCA + scaler objects to: pca_and_scaler.pkl\n",
      "[*] Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "make_observations_22d.py\n",
    "\n",
    "Produces observations_22d.npy = [PCA20 components (scaled)] + [recon_scaled] + [rule_scaled]\n",
    "Inputs:\n",
    "  - embeddings.npy               (shape N x D)\n",
    "  - synthetic_nginx_logs.csv     (must contain query, uri; optional: rule_hit_count, rule_severity_max, label)\n",
    "  - reconstruction_errors.csv    (must contain 'index' and 'reconstruction_error')\n",
    "\n",
    "Outputs:\n",
    "  - observations_22d.npy        (N x 22)\n",
    "  - pca_model.npz               (contains PCA components & std scaler params) (optional)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "# -------- CONFIG --------\n",
    "EMBED_NPY = \"embeddings.npy\"\n",
    "LOG_CSV = \"synthetic_nginx_logs.csv\"\n",
    "ERR_CSV = \"reconstruction_errors.csv\"\n",
    "\n",
    "OUT_OBS = \"observations_22d.npy\"\n",
    "OUT_PCA_PICKLE = \"pca_and_scaler.pkl\"   # saves PCA + StandardScaler objects for future inference\n",
    "PCA_DIM = 30\n",
    "WHITEN = False   # if True, PCA will whiten (not recommended usually)\n",
    "SEED = 42\n",
    "\n",
    "# Pattern groups for OWASP-like detection\n",
    "SQLI_PATTERNS = [\n",
    "    r\"(?i)\\b(or|and)\\b\\s+\\d+=\\d+\",\n",
    "    r\"(?i)union\\s+select\",\n",
    "    r\"(?i)select\\s+.+\\s+from\",\n",
    "    r\"(?i)drop\\s+table\",\n",
    "    r\"(?i)--\\s*$\",\n",
    "    r\"(?i)1=1\",\n",
    "]\n",
    "XSS_PATTERNS = [r\"(?i)<script.*?>\", r\"(?i)onerror=|onmouseover=|<img\\s+src\", r\"(?i)alert\\(\"]\n",
    "LFI_PATTERNS = [r\"\\.\\./\", r\"etc/passwd\"]\n",
    "CMD_PATTERNS = [r\";\\s*rm\\s+-rf\", r\"\\|\\s*cat\\s+/etc/passwd\"]\n",
    "COMPILED_PATTERNS = [(SQLI_PATTERNS, 2.0), (XSS_PATTERNS, 1.8), (LFI_PATTERNS, 1.5), (CMD_PATTERNS, 2.0)]\n",
    "PATTERN_MAX_SCORE = 5.0\n",
    "\n",
    "def pattern_score(text: str) -> float:\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return 0.0\n",
    "    t = text.lower()\n",
    "    score = 0.0\n",
    "    for pats, w in COMPILED_PATTERNS:\n",
    "        for p in pats:\n",
    "            if re.search(p, t):\n",
    "                score += w\n",
    "                break\n",
    "    return float(min(score, PATTERN_MAX_SCORE))\n",
    "\n",
    "def l2_norm_rows(x: np.ndarray) -> np.ndarray:\n",
    "    norms = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1.0\n",
    "    return x / norms\n",
    "\n",
    "def main():\n",
    "    # checks\n",
    "    for f in (EMBED_NPY, LOG_CSV, ERR_CSV):\n",
    "        if not os.path.exists(f):\n",
    "            raise FileNotFoundError(f\"Required file not found: {f}\")\n",
    "\n",
    "    # load\n",
    "    print(\"[*] Loading embeddings...\")\n",
    "    embeddings = np.load(EMBED_NPY)    # shape (N, D)\n",
    "    N, D = embeddings.shape\n",
    "    print(f\"    embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "    print(\"[*] Loading logs CSV...\")\n",
    "    df = pd.read_csv(LOG_CSV, dtype=str).fillna(\"\")\n",
    "    if len(df) != N:\n",
    "        # Try to proceed but warn\n",
    "        print(f\"Warning: CSV rows ({len(df)}) != embeddings rows ({N}). Will attempt to align by index and truncate/pad as needed.\")\n",
    "    # Ensure needed columns exist\n",
    "    if \"query\" not in df.columns:\n",
    "        df[\"query\"] = \"\"\n",
    "    if \"uri\" not in df.columns:\n",
    "        df[\"uri\"] = \"\"\n",
    "    # numeric rule fields optional\n",
    "    if \"rule_hit_count\" not in df.columns:\n",
    "        df[\"rule_hit_count\"] = \"0\"\n",
    "    if \"rule_severity_max\" not in df.columns:\n",
    "        df[\"rule_severity_max\"] = \"0\"\n",
    "\n",
    "    print(\"[*] Loading reconstruction errors CSV...\")\n",
    "    df_errs = pd.read_csv(ERR_CSV).fillna(0.0)\n",
    "    if \"index\" not in df_errs.columns or \"reconstruction_error\" not in df_errs.columns:\n",
    "        raise ValueError(\"reconstruction_errors.csv must contain 'index' and 'reconstruction_error' columns\")\n",
    "\n",
    "    # Align reconstruction_error to df rows by index if possible, otherwise use position\n",
    "    recon_series = df_errs.set_index(\"index\")[\"reconstruction_error\"]\n",
    "    if len(recon_series) >= len(df):\n",
    "        df[\"reconstruction_error\"] = recon_series.values[:len(df)]\n",
    "    else:\n",
    "        # join by index (if df has standard 0..N-1)\n",
    "        try:\n",
    "            df = df.join(recon_series, how=\"left\")\n",
    "            df[\"reconstruction_error\"] = df[\"reconstruction_error\"].fillna(0.0)\n",
    "        except Exception:\n",
    "            # last resort: pad with zeros or truncate\n",
    "            recon_vals = recon_series.values\n",
    "            if len(recon_vals) < len(df):\n",
    "                recon_full = np.zeros(len(df), dtype=float)\n",
    "                recon_full[:len(recon_vals)] = recon_vals\n",
    "                df[\"reconstruction_error\"] = recon_full\n",
    "            else:\n",
    "                df[\"reconstruction_error\"] = recon_vals[:len(df)]\n",
    "\n",
    "    # If df has more/less rows than embeddings, align to min length\n",
    "    M = min(len(df), N)\n",
    "    if M != N:\n",
    "        print(f\"Adjusting to min rows: {M} (min of embeddings and CSV)\")\n",
    "        embeddings = embeddings[:M]\n",
    "        df = df.iloc[:M].reset_index(drop=True)\n",
    "        N = M\n",
    "\n",
    "    # 1) L2-normalize embeddings\n",
    "    emb_norm = l2_norm_rows(embeddings.astype(np.float32))\n",
    "    print(\"[*] Embeddings L2-normalized\")\n",
    "\n",
    "    # 2) PCA -> PCA_DIM\n",
    "    print(f\"[*] Running PCA -> {PCA_DIM} components ...\")\n",
    "    pca = PCA(n_components=PCA_DIM, whiten=WHITEN, random_state=SEED)\n",
    "    pca_feats = pca.fit_transform(emb_norm)   # shape (N, PCA_DIM)\n",
    "    print(\"    PCA done. Explained variance sum:\", float(np.sum(pca.explained_variance_ratio_)))\n",
    "\n",
    "    # 3) standardize PCA outputs (zero mean unit var) then min-max to [-1,1]\n",
    "    scaler = StandardScaler()\n",
    "    pca_std = scaler.fit_transform(pca_feats)  # zero mean, unit var\n",
    "    # min-max to [-1,1]\n",
    "    pmin = pca_std.min(axis=0)\n",
    "    pmax = pca_std.max(axis=0)\n",
    "    prange = np.where((pmax - pmin) == 0, 1.0, (pmax - pmin))\n",
    "    pca_scaled = 2.0 * (pca_std - pmin) / prange - 1.0\n",
    "    print(\"[*] PCA standardized and scaled to [-1,1]\")\n",
    "\n",
    "    # 4) pattern score (OWASP-like) on query + uri\n",
    "    print(\"[*] Computing pattern scores from query+uri ...\")\n",
    "    combined_text = (df[\"query\"].fillna(\"\") + \" \" + df[\"uri\"].fillna(\"\")).astype(str)\n",
    "    pattern_scores = combined_text.apply(pattern_score).values  # 0..PATTERN_MAX_SCORE\n",
    "    pattern_scores_norm = np.clip(pattern_scores / PATTERN_MAX_SCORE, 0.0, 1.0)\n",
    "\n",
    "    # 5) numeric rule features -> min-max [0,1]\n",
    "    rule_hit = pd.to_numeric(df[\"rule_hit_count\"], errors=\"coerce\").fillna(0.0).astype(float).values\n",
    "    rule_sev = pd.to_numeric(df[\"rule_severity_max\"], errors=\"coerce\").fillna(0.0).astype(float).values\n",
    "    rule_numeric = np.vstack([rule_hit, rule_sev]).T\n",
    "    rule_scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "    # safe fit: if constant columns, MinMaxScaler will produce zeros\n",
    "    try:\n",
    "        rule_numeric_scaled = rule_scaler.fit_transform(rule_numeric)\n",
    "    except Exception:\n",
    "        # fallback to manual normalization\n",
    "        rule_numeric_scaled = np.zeros_like(rule_numeric)\n",
    "        for i in range(rule_numeric.shape[1]):\n",
    "            col = rule_numeric[:, i]\n",
    "            mn, mx = col.min(), col.max()\n",
    "            den = mx - mn if (mx - mn) > 1e-12 else 1.0\n",
    "            rule_numeric_scaled[:, i] = (col - mn) / den\n",
    "\n",
    "    rule_numeric_score = np.max(rule_numeric_scaled, axis=1)  # use max of the two normalized numeric signals\n",
    "\n",
    "    # 6) combine numeric rule score and pattern score into rule_score [0,1]\n",
    "    rule_score = np.clip(0.6 * rule_numeric_score + 0.4 * pattern_scores_norm, 0.0, 1.0)\n",
    "\n",
    "    # 7) normalize reconstruction_error -> [0,1]\n",
    "    recon = pd.to_numeric(df[\"reconstruction_error\"], errors=\"coerce\").fillna(0.0).astype(float).values\n",
    "    rmin, rmax = float(recon.min()), float(recon.max())\n",
    "    if (rmax - rmin) < 1e-12:\n",
    "        recon_norm = np.zeros_like(recon)\n",
    "    else:\n",
    "        recon_norm = (recon - rmin) / (rmax - rmin)\n",
    "    recon_norm = np.clip(recon_norm, 0.0, 1.0)\n",
    "\n",
    "    # 8) scale recon_norm and rule_score to [-1,1] for NN stability (optional but matches previous pipeline)\n",
    "    recon_scaled = 2.0 * recon_norm - 1.0\n",
    "    rule_scaled = 2.0 * rule_score - 1.0\n",
    "\n",
    "    # 9) concatenate to produce observations (N x 22)\n",
    "    print(\"[*] Concatenating features into observations (PCA20 + recon + rule) ...\")\n",
    "    observations = np.zeros((N, PCA_DIM + 2), dtype=np.float32)\n",
    "    observations[:, :PCA_DIM] = pca_scaled.astype(np.float32)\n",
    "    observations[:, PCA_DIM] = recon_scaled.astype(np.float32)\n",
    "    observations[:, PCA_DIM + 1] = rule_scaled.astype(np.float32)\n",
    "\n",
    "    # 10) save observations.npy\n",
    "    np.save(OUT_OBS, observations)\n",
    "    print(f\"[+] Saved observations to: {OUT_OBS}  (shape {observations.shape})\")\n",
    "\n",
    "    # 11) optionally persist PCA + scaler + rule_scaler + normalization params for inference\n",
    "    try:\n",
    "        joblib.dump({\n",
    "            \"pca\": pca,\n",
    "            \"std_scaler\": scaler,\n",
    "            \"pca_min\": pmin,\n",
    "            \"pca_max\": pmax,\n",
    "            \"rule_scaler\": rule_scaler,\n",
    "            \"recon_min\": rmin,\n",
    "            \"recon_max\": rmax,\n",
    "        }, OUT_PCA_PICKLE)\n",
    "        print(f\"[+] Saved PCA + scaler objects to: {OUT_PCA_PICKLE}\")\n",
    "    except Exception as e:\n",
    "        print(\"Warning: unable to save PCA objects:\", e)\n",
    "\n",
    "    print(\"[*] Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b2388e-d60b-47aa-9338-9396029a0c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df511f8-c7d2-4cd5-90a6-77622e122cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cef60c1c-2d11-42a2-a4e0-87dc0570931b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Data loaded. N= 7000 obs_dim= 32\n",
      "[*] Starting training: epochs= 20 NUM_STEPS= 4096\n",
      "[Epoch 1/20] avg_rew=0.0534 actions=ALLOW:1969 BLOCK:2127\n",
      "  Offline: acc=0.7989 attack_f1=0.5272 attack_rec=0.3925 attack_prec=0.8027\n",
      "  [+] New best model saved (attack_f1=0.5272)\n",
      "[Epoch 2/20] avg_rew=0.1479 actions=ALLOW:2118 BLOCK:1978\n",
      "  Offline: acc=0.8047 attack_f1=0.5278 attack_rec=0.3820 attack_prec=0.8536\n",
      "  [+] New best model saved (attack_f1=0.5278)\n",
      "[Epoch 3/20] avg_rew=0.2406 actions=ALLOW:2230 BLOCK:1866\n",
      "  Offline: acc=0.8037 attack_f1=0.4982 attack_rec=0.3410 attack_prec=0.9241\n",
      "[Epoch 4/20] avg_rew=0.3638 actions=ALLOW:2434 BLOCK:1662\n",
      "  Offline: acc=0.8177 attack_f1=0.5663 attack_rec=0.4165 attack_prec=0.8843\n",
      "  [+] New best model saved (attack_f1=0.5663)\n",
      "[Epoch 5/20] avg_rew=0.4245 actions=ALLOW:2347 BLOCK:1749\n",
      "  Offline: acc=0.8189 attack_f1=0.5582 attack_rec=0.4005 attack_prec=0.9207\n",
      "[Epoch 6/20] avg_rew=0.4758 actions=ALLOW:2451 BLOCK:1645\n",
      "  Offline: acc=0.8217 attack_f1=0.5691 attack_rec=0.4120 attack_prec=0.9196\n",
      "  [+] New best model saved (attack_f1=0.5691)\n",
      "[Epoch 7/20] avg_rew=0.5068 actions=ALLOW:2399 BLOCK:1697\n",
      "  Offline: acc=0.8226 attack_f1=0.5778 attack_rec=0.4250 attack_prec=0.9023\n",
      "  [+] New best model saved (attack_f1=0.5778)\n",
      "[Epoch 8/20] avg_rew=0.5104 actions=ALLOW:2406 BLOCK:1690\n",
      "  Offline: acc=0.8266 attack_f1=0.5907 attack_rec=0.4380 attack_prec=0.9068\n",
      "  [+] New best model saved (attack_f1=0.5907)\n",
      "[Epoch 9/20] avg_rew=0.5283 actions=ALLOW:2275 BLOCK:1821\n",
      "  Offline: acc=0.8230 attack_f1=0.5708 attack_rec=0.4120 attack_prec=0.9290\n",
      "[Epoch 10/20] avg_rew=0.5621 actions=ALLOW:2434 BLOCK:1662\n",
      "  Offline: acc=0.8207 attack_f1=0.5573 attack_rec=0.3950 attack_prec=0.9461\n",
      "[Epoch 11/20] avg_rew=0.5776 actions=ALLOW:2505 BLOCK:1591\n",
      "  Offline: acc=0.8290 attack_f1=0.6051 attack_rec=0.4585 attack_prec=0.8894\n",
      "  [+] New best model saved (attack_f1=0.6051)\n",
      "[Epoch 12/20] avg_rew=0.5671 actions=ALLOW:2334 BLOCK:1762\n",
      "  Offline: acc=0.8303 attack_f1=0.6133 attack_rec=0.4710 attack_prec=0.8787\n",
      "  [+] New best model saved (attack_f1=0.6133)\n",
      "[Epoch 13/20] avg_rew=0.5864 actions=ALLOW:2230 BLOCK:1866\n",
      "  Offline: acc=0.8304 attack_f1=0.6132 attack_rec=0.4705 attack_prec=0.8803\n",
      "[Epoch 14/20] avg_rew=0.5899 actions=ALLOW:2195 BLOCK:1901\n",
      "  Offline: acc=0.8294 attack_f1=0.6065 attack_rec=0.4600 attack_prec=0.8897\n",
      "[Epoch 15/20] avg_rew=0.5880 actions=ALLOW:2249 BLOCK:1847\n",
      "  Offline: acc=0.8286 attack_f1=0.6011 attack_rec=0.4520 attack_prec=0.8968\n",
      "[Epoch 16/20] avg_rew=0.6000 actions=ALLOW:2318 BLOCK:1778\n",
      "  Offline: acc=0.8329 attack_f1=0.6364 attack_rec=0.5120 attack_prec=0.8407\n",
      "  [+] New best model saved (attack_f1=0.6364)\n",
      "[Epoch 17/20] avg_rew=0.5850 actions=ALLOW:2056 BLOCK:2040\n",
      "  Offline: acc=0.8329 attack_f1=0.6304 attack_rec=0.4990 attack_prec=0.8559\n",
      "[Epoch 18/20] avg_rew=0.5930 actions=ALLOW:2109 BLOCK:1987\n",
      "  Offline: acc=0.8344 attack_f1=0.6422 attack_rec=0.5200 attack_prec=0.8394\n",
      "  [+] New best model saved (attack_f1=0.6422)\n",
      "[Epoch 19/20] avg_rew=0.5886 actions=ALLOW:2054 BLOCK:2042\n",
      "  Offline: acc=0.8331 attack_f1=0.6301 attack_rec=0.4975 attack_prec=0.8592\n",
      "[Epoch 20/20] avg_rew=0.6085 actions=ALLOW:2176 BLOCK:1920\n",
      "  Offline: acc=0.8306 attack_f1=0.6091 attack_rec=0.4620 attack_prec=0.8936\n",
      "[*] Training finished in 4.11 minutes. Best attack_f1=0.6422\n",
      "[*] Final model saved to ppo_policy_final.pt, best model to ppo_policy_best.pt\n",
      "\n",
      "=== Final Offline Metrics (final model) ===\n",
      "Accuracy: 0.8306\n",
      "Attack precision: 0.8936\n",
      "Attack recall   : 0.4620\n",
      "Attack F1       : 0.6091\n",
      "Confusion matrix:\n",
      " [[4890  110]\n",
      " [1076  924]]\n",
      "[+] Saved actions to logs_with_policy_actions.csv\n",
      "[+] Best model (by attack_f1) stored at: ppo_policy_best.pt\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ppo_pipeline_complete.py\n",
    "\n",
    "Single-file pipeline (train + eval + save/load) for custom PPO on 22-d WAF features (PCA20 + recon + rule).\n",
    "\n",
    "Prereqs:\n",
    "  pip install torch numpy pandas scikit-learn gym\n",
    "\n",
    "Files expected in working dir:\n",
    "  - observations_22d.npy        (shape [N,22])\n",
    "  - synthetic_nginx_logs.csv    (must contain 'label' column)\n",
    "Optional:\n",
    "  - reconstruction_errors.csv (only needed if you want to re-create observations)\n",
    "\n",
    "Outputs:\n",
    "  - ppo_policy_final.pt\n",
    "  - ppo_policy_best.pt\n",
    "  - logs_with_policy_actions.csv\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG (tweak as needed)\n",
    "# ----------------------------\n",
    "OBS_NPY = \"observations_22d.npy\"\n",
    "LOG_CSV = \"synthetic_nginx_logs.csv\"\n",
    "\n",
    "OUT_MODEL_FINAL = \"ppo_policy_final.pt\"\n",
    "OUT_MODEL_BEST = \"ppo_policy_best.pt\"\n",
    "OUT_CSV = \"logs_with_policy_actions.csv\"\n",
    "\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# training hyperparams\n",
    "TOTAL_EPOCHS = 20\n",
    "NUM_STEPS = 4096\n",
    "PPO_EPOCHS = 6\n",
    "MINI_BATCH_SIZE = 128\n",
    "GAMMA = 0.995\n",
    "GAE_LAMBDA = 0.95\n",
    "CLIP_EPS = 0.2\n",
    "LR = 3e-4\n",
    "VALUE_COEF = 0.5\n",
    "ENTROPY_COEF = 0.01\n",
    "MAX_GRAD_NORM = 0.5\n",
    "\n",
    "# env & reward shaping\n",
    "EPISODE_LEN = 512\n",
    "ATTACK_BIAS = 0.9\n",
    "HIGH_THRESH = 0.55\n",
    "MID_THRESH = 0.30\n",
    "\n",
    "HIDDEN_SIZES = [128, 128]\n",
    "VERBOSE = True\n",
    "\n",
    "# reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities / model helpers\n",
    "# ----------------------------\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int = 2, hidden: Tuple[int, ...] = (128, 128)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = obs_dim\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev = h\n",
    "        self.shared = nn.Sequential(*layers)\n",
    "        self.pi = nn.Linear(prev, action_dim)\n",
    "        self.v = nn.Linear(prev, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        h = self.shared(x)\n",
    "        return self.pi(h), self.v(h).squeeze(-1)\n",
    "\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, obs_dim: int, lr: float = LR, hidden=(128, 128)):\n",
    "        self.net = ActorCritic(obs_dim, action_dim=2, hidden=hidden).to(DEVICE)\n",
    "        self.opt = optim.Adam(self.net.parameters(), lr=lr)\n",
    "\n",
    "    def act(self, obs_np: np.ndarray) -> Tuple[int, float, float]:\n",
    "        t = torch.tensor(obs_np, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        logits, val = self.net(t)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        a = dist.sample()\n",
    "        return int(a.item()), float(dist.log_prob(a).item()), float(val.item())\n",
    "\n",
    "    def get_logits_values(self, batch_obs: np.ndarray):\n",
    "        t = torch.tensor(batch_obs, dtype=torch.float32, device=DEVICE)\n",
    "        logits, vals = self.net(t)\n",
    "        return logits, vals\n",
    "\n",
    "    def update(self, obs_buf, act_buf, adv_buf, ret_buf, logp_buf,\n",
    "               ppo_epochs=PPO_EPOCHS, mini_batch=MINI_BATCH_SIZE):\n",
    "        obs = torch.tensor(obs_buf, dtype=torch.float32, device=DEVICE)\n",
    "        acts = torch.tensor(act_buf, dtype=torch.long, device=DEVICE)\n",
    "        old_logp = torch.tensor(logp_buf, dtype=torch.float32, device=DEVICE)\n",
    "        advs = torch.tensor(adv_buf, dtype=torch.float32, device=DEVICE)\n",
    "        rets = torch.tensor(ret_buf, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "        advs = (advs - advs.mean()) / (advs.std() + 1e-9)\n",
    "        N = obs.shape[0]\n",
    "        idxs = np.arange(N)\n",
    "        for _ in range(ppo_epochs):\n",
    "            np.random.shuffle(idxs)\n",
    "            for start in range(0, N, mini_batch):\n",
    "                mb = idxs[start:start + mini_batch]\n",
    "                mb_obs = obs[mb]\n",
    "                mb_acts = acts[mb]\n",
    "                mb_advs = advs[mb]\n",
    "                mb_rets = rets[mb]\n",
    "                mb_old_logp = old_logp[mb]\n",
    "\n",
    "                logits, vals = self.net(mb_obs)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                dist = Categorical(probs)\n",
    "                mb_logp = dist.log_prob(mb_acts)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                ratio = torch.exp(mb_logp - mb_old_logp)\n",
    "                surr1 = ratio * mb_advs\n",
    "                surr2 = torch.clamp(ratio, 1.0 - CLIP_EPS, 1.0 + CLIP_EPS) * mb_advs\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                value_loss = ((vals - mb_rets) ** 2).mean()\n",
    "\n",
    "                loss = policy_loss + VALUE_COEF * value_loss - ENTROPY_COEF * entropy\n",
    "\n",
    "                self.opt.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.net.parameters(), MAX_GRAD_NORM)\n",
    "                self.opt.step()\n",
    "\n",
    "    def save(self, path: str):\n",
    "        torch.save(self.net.state_dict(), path)\n",
    "\n",
    "    def load(self, path: str):\n",
    "        self.net.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Environment (weighted sampling)\n",
    "# ----------------------------\n",
    "class WafPPOEnvRandom(gym.Env):\n",
    "    def __init__(self, observations: np.ndarray, labels: np.ndarray,\n",
    "                 episode_len: int = EPISODE_LEN, attack_bias: float = ATTACK_BIAS):\n",
    "        super().__init__()\n",
    "        self.observations = observations.astype(np.float32)\n",
    "        self.labels = labels.astype(int)\n",
    "        self.n = observations.shape[0]\n",
    "        self.episode_len = int(episode_len)\n",
    "        alpha = 1.0 + 50.0 * float(np.clip(attack_bias, 0.0, 1.0))\n",
    "        weights = np.ones(self.n, dtype=np.float32)\n",
    "        weights[self.labels == 1] *= alpha\n",
    "        self.sampling_weights = weights / np.sum(weights)\n",
    "        self.current_step = 0\n",
    "        self.current_idx = 0\n",
    "        self.observation_space = spaces.Box(low=-5.0, high=5.0, shape=(self.observations.shape[1],), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self._seed = None\n",
    "        self.centroid_cached = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        if seed is None:\n",
    "            seed = np.random.randint(0, 2 ** 31 - 1)\n",
    "        self._seed = int(seed)\n",
    "        np.random.seed(self._seed)\n",
    "        random.seed(self._seed)\n",
    "        return [self._seed]\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.seed(int(seed))\n",
    "        self.current_step = 0\n",
    "        self.current_idx = int(np.random.choice(self.n, p=self.sampling_weights))\n",
    "        return self.observations[self.current_idx].copy(), {}\n",
    "\n",
    "    def _ensure_centroid(self):\n",
    "        # centroid for pseudo-embedding anomaly computed on-the-fly for speed\n",
    "        if self.centroid_cached is None:\n",
    "            benign_mask = (self.labels == 0)\n",
    "            if benign_mask.sum() == 0:\n",
    "                self.centroid_cached = np.zeros(self.observations.shape[1] - 2, dtype=np.float32)\n",
    "            else:\n",
    "                self.centroid_cached = self.observations[benign_mask, :-2].mean(axis=0)\n",
    "                norm = np.linalg.norm(self.centroid_cached)\n",
    "                if norm > 0:\n",
    "                    self.centroid_cached = self.centroid_cached / norm\n",
    "\n",
    "    def step(self, action):\n",
    "        obs = self.observations[self.current_idx]\n",
    "        recon_scaled = float(obs[-2]); rule_scaled = float(obs[-1])\n",
    "        recon_val = (recon_scaled + 1.0) / 2.0\n",
    "        rule_val = (rule_scaled + 1.0) / 2.0\n",
    "\n",
    "        self._ensure_centroid()\n",
    "        emb_vec = obs[:-2].astype(np.float32)\n",
    "        denom = (np.linalg.norm(emb_vec) + 1e-9)\n",
    "        cos = float(np.dot(emb_vec, self.centroid_cached) / denom)\n",
    "        emb_anom = float((1.0 - cos) / 2.0)\n",
    "\n",
    "        combined_anom = 0.6 * recon_val + 0.3 * rule_val + 0.1 * emb_anom\n",
    "        combined_anom = float(np.clip(combined_anom, 0.0, 1.0))\n",
    "\n",
    "        # stronger shaped reward\n",
    "        if combined_anom >= HIGH_THRESH:\n",
    "            reward = 3.0 if action == 1 else -2.0\n",
    "        elif combined_anom >= MID_THRESH:\n",
    "            reward = 1.0 if action == 1 else -0.8\n",
    "        else:\n",
    "            reward = 1.0 if action == 0 else -0.9\n",
    "\n",
    "        self.current_idx = int(np.random.choice(self.n, p=self.sampling_weights))\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.episode_len\n",
    "        next_obs = self.observations[self.current_idx].copy() if not done else np.zeros_like(self.observations[0], dtype=np.float32)\n",
    "        return next_obs, float(reward), done, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Rollout collection & GAE\n",
    "# ----------------------------\n",
    "def collect_rollout(agent: PPOAgent, env: WafPPOEnvRandom, num_steps: int):\n",
    "    obs_buf, act_buf, rew_buf, val_buf, logp_buf, done_buf = [], [], [], [], [], []\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(num_steps):\n",
    "        a, logp, val = agent.act(obs)\n",
    "        next_obs, rew, done, _ = env.step(a)\n",
    "        obs_buf.append(obs.copy()); act_buf.append(a); rew_buf.append(rew)\n",
    "        val_buf.append(val); logp_buf.append(logp); done_buf.append(done)\n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            obs, _ = env.reset()\n",
    "    return (np.array(obs_buf), np.array(act_buf), np.array(rew_buf), np.array(val_buf), np.array(logp_buf), np.array(done_buf))\n",
    "\n",
    "\n",
    "def compute_gae(rewards, values, dones, last_val, gamma=GAMMA, lam=GAE_LAMBDA):\n",
    "    T = len(rewards)\n",
    "    adv = np.zeros(T, dtype=np.float32)\n",
    "    last_gae = 0.0\n",
    "    for t in reversed(range(T)):\n",
    "        if t == T - 1:\n",
    "            next_non_term = 0.0 if dones[t] else 1.0\n",
    "            next_val = last_val\n",
    "        else:\n",
    "            next_non_term = 0.0 if dones[t + 1] else 1.0\n",
    "            next_val = values[t + 1]\n",
    "        delta = rewards[t] + gamma * next_val * next_non_term - values[t]\n",
    "        last_gae = delta + gamma * lam * next_non_term * last_gae\n",
    "        adv[t] = last_gae\n",
    "    returns = adv + values\n",
    "    return adv, returns\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Offline evaluation\n",
    "# ----------------------------\n",
    "def evaluate_policy(agent: PPOAgent, observations: np.ndarray, labels: np.ndarray) -> Dict[str, Any]:\n",
    "    actions = []\n",
    "    agent.net.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(observations)):\n",
    "            x = torch.tensor(observations[i], dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "            logits, _ = agent.net(x)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            action = int(torch.argmax(probs, dim=-1).cpu().numpy()[0])\n",
    "            actions.append(action)\n",
    "    actions = np.array(actions, dtype=int)\n",
    "    acc = accuracy_score(labels, actions)\n",
    "    p, r, f, _ = precision_recall_fscore_support(labels, actions, average=None, zero_division=0)\n",
    "    attack_prec = float(p[1]) if len(p) > 1 else 0.0\n",
    "    attack_rec = float(r[1]) if len(r) > 1 else 0.0\n",
    "    attack_f1 = float(f[1]) if len(f) > 1 else 0.0\n",
    "    cm = confusion_matrix(labels, actions)\n",
    "    return {\"accuracy\": acc, \"attack_prec\": attack_prec, \"attack_rec\": attack_rec, \"attack_f1\": attack_f1, \"confusion_matrix\": cm, \"actions\": actions}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Training loop (main)\n",
    "# ----------------------------\n",
    "def train(observations: np.ndarray, labels: np.ndarray):\n",
    "    N, obs_dim = observations.shape\n",
    "    env = WafPPOEnvRandom(observations, labels, episode_len=EPISODE_LEN, attack_bias=ATTACK_BIAS)\n",
    "    agent = PPOAgent(obs_dim=obs_dim, lr=LR, hidden=tuple(HIDDEN_SIZES))\n",
    "\n",
    "    best_f1 = -1.0\n",
    "    best_path = OUT_MODEL_BEST\n",
    "\n",
    "    print(\"[*] Starting training: epochs=\", TOTAL_EPOCHS, \"NUM_STEPS=\", NUM_STEPS)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, TOTAL_EPOCHS + 1):\n",
    "        obs_b, act_b, rew_b, val_b, logp_b, done_b = collect_rollout(agent, env, NUM_STEPS)\n",
    "        with torch.no_grad():\n",
    "            _, last_val = agent.get_logits_values(obs_b[-1:].astype(np.float32))\n",
    "            last_val = float(last_val.cpu().numpy()[0])\n",
    "        adv_b, ret_b = compute_gae(rew_b, val_b, done_b, last_val)\n",
    "        agent.update(obs_b, act_b, adv_b, ret_b, logp_b)\n",
    "\n",
    "        avg_rew = float(np.mean(rew_b))\n",
    "        action_counts = np.bincount(act_b, minlength=2)\n",
    "        if VERBOSE:\n",
    "            print(f\"[Epoch {epoch}/{TOTAL_EPOCHS}] avg_rew={avg_rew:.4f} actions=ALLOW:{action_counts[0]} BLOCK:{action_counts[1]}\")\n",
    "\n",
    "        # offline evaluation and model saving by attack_f1\n",
    "        metrics = evaluate_policy(agent, observations, labels)\n",
    "        if VERBOSE:\n",
    "            print(f\"  Offline: acc={metrics['accuracy']:.4f} attack_f1={metrics['attack_f1']:.4f} attack_rec={metrics['attack_rec']:.4f} attack_prec={metrics['attack_prec']:.4f}\")\n",
    "\n",
    "        if metrics[\"attack_f1\"] > best_f1:\n",
    "            best_f1 = metrics[\"attack_f1\"]\n",
    "            agent.save(best_path)\n",
    "            if VERBOSE:\n",
    "                print(f\"  [+] New best model saved (attack_f1={best_f1:.4f})\")\n",
    "\n",
    "    elapsed = (time.time() - start_time) / 60.0\n",
    "    print(f\"[*] Training finished in {elapsed:.2f} minutes. Best attack_f1={best_f1:.4f}\")\n",
    "    agent.save(OUT_MODEL_FINAL)\n",
    "    print(f\"[*] Final model saved to {OUT_MODEL_FINAL}, best model to {best_path}\")\n",
    "    return agent, best_path\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Save / Load convenience\n",
    "# ----------------------------\n",
    "def save_model(agent: PPOAgent, path: str):\n",
    "    agent.save(path)\n",
    "    print(f\"[+] Model saved to {path}\")\n",
    "\n",
    "\n",
    "def load_model(path: str, obs_dim: int) -> PPOAgent:\n",
    "    agent = PPOAgent(obs_dim=obs_dim, lr=LR, hidden=tuple(HIDDEN_SIZES))\n",
    "    agent.load(path)\n",
    "    agent.net.to(DEVICE)\n",
    "    agent.net.eval()\n",
    "    print(f\"[+] Loaded model from {path}\")\n",
    "    return agent\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main entry\n",
    "# ----------------------------\n",
    "def main():\n",
    "    if not os.path.exists(OBS_NPY):\n",
    "        raise FileNotFoundError(f\"Missing observations file: {OBS_NPY}\")\n",
    "    if not os.path.exists(LOG_CSV):\n",
    "        raise FileNotFoundError(f\"Missing log CSV: {LOG_CSV}\")\n",
    "\n",
    "    observations = np.load(OBS_NPY).astype(np.float32)\n",
    "    df_logs = pd.read_csv(LOG_CSV, dtype=str).fillna(\"\")\n",
    "    if \"label\" not in df_logs.columns:\n",
    "        df_logs[\"label\"] = 0\n",
    "    df_logs[\"label\"] = pd.to_numeric(df_logs[\"label\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    labels = df_logs[\"label\"].values\n",
    "\n",
    "    print(\"[*] Data loaded. N=\", observations.shape[0], \"obs_dim=\", observations.shape[1])\n",
    "    agent, best_path = train(observations, labels)\n",
    "\n",
    "    # final evaluation using the final agent\n",
    "    final_metrics = evaluate_policy(agent, observations, labels)\n",
    "    print(\"\\n=== Final Offline Metrics (final model) ===\")\n",
    "    print(f\"Accuracy: {final_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Attack precision: {final_metrics['attack_prec']:.4f}\")\n",
    "    print(f\"Attack recall   : {final_metrics['attack_rec']:.4f}\")\n",
    "    print(f\"Attack F1       : {final_metrics['attack_f1']:.4f}\")\n",
    "    print(\"Confusion matrix:\\n\", final_metrics[\"confusion_matrix\"])\n",
    "\n",
    "    # save actions to CSV\n",
    "    actions = final_metrics[\"actions\"]\n",
    "    df_out = df_logs.copy()\n",
    "    df_out[\"policy_action\"] = actions\n",
    "    df_out.to_csv(OUT_CSV, index=False)\n",
    "    print(f\"[+] Saved actions to {OUT_CSV}\")\n",
    "\n",
    "    # also show best model path\n",
    "    print(f\"[+] Best model (by attack_f1) stored at: {best_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47253c8d-2002-4e03-995a-33852a7c5872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
