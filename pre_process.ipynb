{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e87ac67-b754-4144-8792-5da930c8a5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7000 rows from synthetic_nginx_logs.csv\n",
      "Example fused text (first 5):\n",
      "['GET shop.example.com /product/123 /product/123 1; DROP TABLE users; 243.87.246.107 170.192.56:15901 python-requests/2.25.1 - HTTP/1.1 500 application/json - - - 2025-12-10T20:48:08.841422', 'GET alpha-triradiate-adalberto.ngrok-free.dev /profile /profile q=879 197.1.74.78 74.85.97:8854 PostmanRuntime/7.28.4 - HTTP/1.1 200 application/json - - - 2025-12-11T05:39:32.841422', 'POST shop.example.com /dashboard /dashboard q=442 196.12.196.145 172.18.0.5:80 curl/7.68.0 https://google.com HTTP/1.1 200 application/x-www-form-urlencoded - TLSv1.3 ECDHE-ECDSA-AES256-GCM-SHA384 2025-12-11T02:38:04.841422', 'POST shop.example.com /dashboard /dashboard - 122.155.200.122 54.113.254:9409 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/143.0.0.0 Safari/537.36 - HTTP/2.0 200 text/html - TLSv1.2 ECDHE-ECDSA-AES256-GCM-SHA384 2025-12-11T16:09:13.841422', 'GET shop.example.com /\"_onmouseover=\"alert(\\'XSS\\')\" /\"_onmouseover=\"alert(\\'XSS\\')\" - 240.65.69.63 47.171.233:28934 sqlmap/1.5.6 (http://sqlmap.org) - HTTP/1.1 200 application/json 524:26cf:2bab:636b:40ef:9f75:6c2b:9fcb - - 2025-12-11T19:13:56.841422']\n",
      "\n",
      "✔️ Saved tokenized tensors to tokenized_logs.pt\n",
      "input_ids shape: torch.Size([7000, 128])\n",
      "attention_mask shape: torch.Size([7000, 128])\n",
      "labels shape: torch.Size([7000])\n"
     ]
    }
   ],
   "source": [
    "# tokenized_preprocess_nginx_logs.py\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ------------------------------\n",
    "# CONFIG\n",
    "# ------------------------------\n",
    "CSV_FILE = \"synthetic_nginx_logs.csv\"   # point to your CSV (change if needed)\n",
    "MAX_LENGTH = 128\n",
    "OUTPUT_PT = \"tokenized_logs.pt\"\n",
    "TOKENIZER_NAME = \"distilroberta-base\"  # keep same\n",
    "\n",
    "# ------------------------------\n",
    "# LOAD DATA\n",
    "# ------------------------------\n",
    "df = pd.read_csv(CSV_FILE, dtype=str)  # read everything as string to avoid dtype surprises\n",
    "print(f\"Loaded {len(df)} rows from {CSV_FILE}\")\n",
    "\n",
    "# ------------------------------\n",
    "# ENSURE EXPECTED FIELDS (from your nginx log_format)\n",
    "# ------------------------------\n",
    "expected_fields = [\n",
    "    \"time\",\"msec\",\"client_ip\",\"host\",\"method\",\"uri\",\"path\",\"query\",\"protocol\",\n",
    "    \"status\",\"body_bytes_sent\",\"request_length\",\"request_time\",\"upstream_response_time\",\n",
    "    \"upstream_addr\",\"user_agent\",\"referer\",\"content_type\",\"x_forwarded_for\",\n",
    "    \"ssl_protocol\",\"ssl_cipher\"\n",
    "]\n",
    "\n",
    "# Add any missing expected columns as empty strings (keeps schema stable)\n",
    "for c in expected_fields:\n",
    "    if c not in df.columns:\n",
    "        df[c] = \"\"\n",
    "\n",
    "# If there is no label column, create a default (0 = benign). If present, cast to int.\n",
    "if \"label\" not in df.columns:\n",
    "    df[\"label\"] = \"0\"\n",
    "else:\n",
    "    # normalize label column (some CSVs may have strings)\n",
    "    df[\"label\"] = df[\"label\"].fillna(\"0\").astype(str)\n",
    "\n",
    "# Replace NaN and None in all fields with empty string\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# ------------------------------\n",
    "# TEXT FUSION FUNCTION (exact fields)\n",
    "# ------------------------------\n",
    "def row_to_text_exact(row):\n",
    "    # Choose fields that are most informative for text models.\n",
    "    # Keep order reasonable: method, host, uri, path, query, client_ip, upstream_addr,\n",
    "    # user_agent, referer, protocol, status, content_type, ssl info, timestamp\n",
    "    parts = [\n",
    "        row.get(\"method\", \"\"),\n",
    "        row.get(\"host\", \"\"),\n",
    "        row.get(\"uri\", \"\"),\n",
    "        row.get(\"path\", \"\"),\n",
    "        row.get(\"query\", \"\"),\n",
    "        row.get(\"client_ip\", \"\"),\n",
    "        row.get(\"upstream_addr\", \"\"),\n",
    "        row.get(\"user_agent\", \"\"),\n",
    "        row.get(\"referer\", \"\"),\n",
    "        row.get(\"protocol\", \"\"),\n",
    "        row.get(\"status\", \"\"),\n",
    "        row.get(\"content_type\", \"\"),\n",
    "        row.get(\"x_forwarded_for\", \"\"),\n",
    "        row.get(\"ssl_protocol\", \"\"),\n",
    "        row.get(\"ssl_cipher\", \"\"),\n",
    "        row.get(\"time\", \"\")\n",
    "    ]\n",
    "    # filter out empty items and join with a single space\n",
    "    return \" \".join([str(p).strip() for p in parts if str(p).strip() != \"\"])\n",
    "\n",
    "# Build text column\n",
    "df[\"text\"] = df.apply(row_to_text_exact, axis=1)\n",
    "\n",
    "print(\"Example fused text (first 5):\")\n",
    "print(df[\"text\"].head(5).to_list())\n",
    "\n",
    "# ------------------------------\n",
    "# TOKENIZER\n",
    "# ------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "encodings = tokenizer(\n",
    "    list(df[\"text\"]),\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# LABELS\n",
    "# ------------------------------\n",
    "# Convert label strings to integers (safe cast)\n",
    "labels = torch.tensor(df[\"label\"].astype(int).values, dtype=torch.long)\n",
    "\n",
    "# ------------------------------\n",
    "# SAVE TENSORS\n",
    "# ------------------------------\n",
    "dataset = {\n",
    "    \"input_ids\": encodings[\"input_ids\"],\n",
    "    \"attention_mask\": encodings[\"attention_mask\"],\n",
    "    \"labels\": labels,\n",
    "    # optional: keep original indices / metadata for debugging\n",
    "    \"meta_index\": torch.arange(len(df), dtype=torch.long),\n",
    "}\n",
    "\n",
    "torch.save(dataset, OUTPUT_PT)\n",
    "\n",
    "print(f\"\\n✔️ Saved tokenized tensors to {OUTPUT_PT}\")\n",
    "print(f\"input_ids shape: {dataset['input_ids'].shape}\")\n",
    "print(f\"attention_mask shape: {dataset['attention_mask'].shape}\")\n",
    "print(f\"labels shape: {dataset['labels'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89350c76-a404-49c9-bbcd-f53820cde87e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
